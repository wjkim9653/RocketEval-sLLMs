model: Qwen/Qwen2.5-0.5B-Instruct    # The model used, can be a local path or a huggingface model repo
# model: meta-llama/Meta-Llama-3-8B-Instruct
# model: wjkim9653/Llama-3-8B-CheckGen-v0-2  # For Checklist Generation ONLY
trust_remote_code: true              
tensor_parallel_size: 1              
gpu_memory_utilization: 0.9          
# max_model_len: 4096                  # Increase the maximum context length only when necessary.
max_model_len: 6144  # WildBench(Grading Only) && Arena-Hard && AlpacaEval
# max_model_len: 4096  # MT-Bench
dtype: auto                          
seed: 0                              
max_num_seqs: 512                    # The maximum number of sequences to use. Both too high or too low will lead to efficiency degradation.
enable_prefix_caching: true          # Whether to enable prefix caching. Recommended for grading tasks.